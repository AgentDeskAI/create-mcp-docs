/**
 * Advanced chunking service with support for Late Chunking
 *
 * Implements the "Late Chunking" approach described by Jina AI where
 * documents are processed through the entire context before chunking,
 * preserving contextual information across chunk boundaries.
 *
 * @see https://jina.ai/news/late-chunking-in-long-context-embedding-models/
 */

import { Tiktoken } from "js-tiktoken/lite";
import o200k_base from "js-tiktoken/ranks/o200k_base";

const tokenizer = new Tiktoken(o200k_base);

export interface ChunkingConfig {
  /** Chunking strategy to use */
  strategy: "traditional" | "late-chunking" | "semantic" | "sentence";

  /** Target chunk size in tokens */
  chunkSize: number;

  /** Overlap between chunks in tokens */
  chunkOverlap: number;

  /** Minimum chunk size - discard smaller chunks */
  minChunkSize?: number;

  /** Maximum chunk size - force split larger chunks */
  maxChunkSize?: number;

  /** For late chunking: whether to use contextual embeddings */
  useContextualEmbeddings?: boolean;

  /** Custom separators for semantic chunking */
  semanticSeparators?: string[];
}

export interface DocumentChunk {
  /** Chunk content */
  content: string;

  /** Position in original document (0-based) */
  index: number;

  /** Start position in original text */
  startOffset: number;

  /** End position in original text */
  endOffset: number;

  /** Token count for this chunk */
  tokenCount: number;

  /** Metadata about the chunk */
  metadata: {
    /** Original document URL */
    url: string;

    /** Chunk type (paragraph, sentence, etc.) */
    type: "paragraph" | "sentence" | "semantic" | "token-based";

    /** Context information for late chunking */
    context?: {
      /** Preceding context summary */
      before?: string;

      /** Following context summary */
      after?: string;

      /** Full document context length */
      documentLength: number;
    };
  };
}

/**
 * Advanced document chunking service
 */
export class ChunkingService {
  private config: ChunkingConfig;

  constructor(config: Partial<ChunkingConfig> = {}) {
    this.config = {
      strategy: "late-chunking",
      chunkSize: 512,
      chunkOverlap: 50,
      minChunkSize: 100,
      maxChunkSize: 1000,
      useContextualEmbeddings: true,
      semanticSeparators: ["\n\n", "\n", ". ", "! ", "? "],
      ...config,
    };
  }

  /**
   * Chunk a document using the configured strategy
   */
  async chunkDocument(content: string, url: string): Promise<DocumentChunk[]> {
    // Pre-process content
    const cleanedContent = this.preprocessContent(content);

    switch (this.config.strategy) {
      case "late-chunking":
        return this.lateChunking(cleanedContent, url);
      case "semantic":
        return this.semanticChunking(cleanedContent, url);
      case "sentence":
        return this.sentenceChunking(cleanedContent, url);
      case "traditional":
      default:
        return this.traditionalChunking(cleanedContent, url);
    }
  }

  /**
   * Late Chunking: Process full document context, then chunk embeddings
   *
   * This preserves contextual information across chunk boundaries by
   * considering the entire document context when creating embeddings.
   */
  private async lateChunking(
    content: string,
    url: string
  ): Promise<DocumentChunk[]> {
    console.log(`ðŸ§  Applying Late Chunking to document: ${url}`);

    // Step 1: Identify semantic boundaries (paragraphs, sections)
    const semanticBoundaries = this.findSemanticBoundaries(content);

    // Step 2: Create overlapping chunks respecting semantic boundaries
    const chunks: DocumentChunk[] = [];
    let currentPosition = 0;
    let chunkIndex = 0;

    while (currentPosition < content.length) {
      // Find the best chunk ending point (respecting semantic boundaries)
      const chunkEnd = this.findOptimalChunkEnd(
        content,
        currentPosition,
        this.config.chunkSize,
        semanticBoundaries
      );

      const chunkContent = content.slice(currentPosition, chunkEnd);
      const tokenCount = tokenizer.encode(chunkContent).length;

      // Skip chunks that are too small
      if (tokenCount < (this.config.minChunkSize || 50)) {
        currentPosition = chunkEnd;
        continue;
      }

      // Create contextual metadata for late chunking
      const context = this.createContextualMetadata(
        content,
        currentPosition,
        chunkEnd,
        url
      );

      chunks.push({
        content: chunkContent,
        index: chunkIndex++,
        startOffset: currentPosition,
        endOffset: chunkEnd,
        tokenCount,
        metadata: {
          url,
          type: "semantic",
          context,
        },
      });

      // Move to next chunk with overlap
      currentPosition = Math.max(
        currentPosition + this.config.chunkSize - this.config.chunkOverlap,
        chunkEnd - this.config.chunkOverlap
      );
    }

    console.log(`ðŸ“ Created ${chunks.length} contextual chunks from document`);
    return chunks;
  }

  /**
   * Traditional chunking: Simple token-based splitting
   */
  private traditionalChunking(
    content: string,
    url: string
  ): Promise<DocumentChunk[]> {
    const chunks: DocumentChunk[] = [];
    const tokens = tokenizer.encode(content);
    let chunkIndex = 0;

    for (
      let i = 0;
      i < tokens.length;
      i += this.config.chunkSize - this.config.chunkOverlap
    ) {
      const chunkTokens = tokens.slice(i, i + this.config.chunkSize);
      const chunkContent = tokenizer.decode(chunkTokens);

      if (chunkTokens.length < (this.config.minChunkSize || 50)) {
        continue;
      }

      chunks.push({
        content: chunkContent,
        index: chunkIndex++,
        startOffset: 0, // Approximate - would need character mapping
        endOffset: chunkContent.length,
        tokenCount: chunkTokens.length,
        metadata: {
          url,
          type: "token-based",
        },
      });
    }

    return Promise.resolve(chunks);
  }

  /**
   * Semantic chunking: Split on natural boundaries (paragraphs, sections)
   */
  private async semanticChunking(
    content: string,
    url: string
  ): Promise<DocumentChunk[]> {
    const chunks: DocumentChunk[] = [];
    const separators = this.config.semanticSeparators || ["\n\n", "\n"];

    // Split by strongest separator first
    let sections = [content];
    for (const separator of separators) {
      const newSections: string[] = [];
      for (const section of sections) {
        newSections.push(...section.split(separator));
      }
      sections = newSections;
    }

    let chunkIndex = 0;
    let currentOffset = 0;

    for (const section of sections) {
      const tokenCount = tokenizer.encode(section).length;

      if (tokenCount < (this.config.minChunkSize || 50)) {
        currentOffset += section.length;
        continue;
      }

      // If section is too large, split it further
      if (tokenCount > this.config.maxChunkSize!) {
        const subChunks = await this.traditionalChunking(section, url);
        chunks.push(
          ...subChunks.map((chunk) => ({
            ...chunk,
            index: chunkIndex++,
            startOffset: currentOffset + chunk.startOffset,
            endOffset: currentOffset + chunk.endOffset,
          }))
        );
      } else {
        chunks.push({
          content: section.trim(),
          index: chunkIndex++,
          startOffset: currentOffset,
          endOffset: currentOffset + section.length,
          tokenCount,
          metadata: {
            url,
            type: "semantic",
          },
        });
      }

      currentOffset += section.length;
    }

    return Promise.resolve(chunks);
  }

  /**
   * Sentence-based chunking: Split on sentence boundaries
   */
  private sentenceChunking(
    content: string,
    url: string
  ): Promise<DocumentChunk[]> {
    // Simple sentence splitting - could be enhanced with NLP libraries
    const sentences = content.match(/[^\.!?]+[\.!?]+/g) || [content];
    const chunks: DocumentChunk[] = [];

    let currentChunk = "";
    let chunkIndex = 0;
    let currentOffset = 0;

    for (const sentence of sentences) {
      const potentialChunk = currentChunk + " " + sentence;
      const tokenCount = tokenizer.encode(potentialChunk).length;

      if (tokenCount > this.config.chunkSize && currentChunk) {
        // Save current chunk
        chunks.push({
          content: currentChunk.trim(),
          index: chunkIndex++,
          startOffset: currentOffset - currentChunk.length,
          endOffset: currentOffset,
          tokenCount: tokenizer.encode(currentChunk).length,
          metadata: {
            url,
            type: "sentence",
          },
        });

        currentChunk = sentence;
      } else {
        currentChunk = potentialChunk;
      }

      currentOffset += sentence.length;
    }

    // Add final chunk
    if (currentChunk.trim()) {
      chunks.push({
        content: currentChunk.trim(),
        index: chunkIndex,
        startOffset: currentOffset - currentChunk.length,
        endOffset: currentOffset,
        tokenCount: tokenizer.encode(currentChunk).length,
        metadata: {
          url,
          type: "sentence",
        },
      });
    }

    return Promise.resolve(chunks);
  }

  /**
   * Clean and preprocess content before chunking
   */
  private preprocessContent(content: string): string {
    return content
      .replace(/\r\n/g, "\n") // Normalize line endings
      .replace(/\n{3,}/g, "\n\n") // Reduce excessive whitespace
      .replace(/[ \t]+/g, " ") // Normalize spaces
      .trim();
  }

  /**
   * Find semantic boundaries in the content (headers, paragraphs, etc.)
   */
  private findSemanticBoundaries(content: string): number[] {
    const boundaries: number[] = [0]; // Start of document

    // Find markdown headers
    const headerRegex = /^#+\s/gm;
    let match;
    while ((match = headerRegex.exec(content)) !== null) {
      boundaries.push(match.index);
    }

    // Find paragraph boundaries
    const paragraphRegex = /\n\n/g;
    while ((match = paragraphRegex.exec(content)) !== null) {
      boundaries.push(match.index);
    }

    boundaries.push(content.length); // End of document
    return [...new Set(boundaries)].sort((a, b) => a - b);
  }

  /**
   * Find optimal chunk ending point respecting semantic boundaries
   */
  private findOptimalChunkEnd(
    content: string,
    start: number,
    targetSize: number,
    boundaries: number[]
  ): number {
    const targetEnd = start + targetSize * 4; // Rough character estimate

    // Find the best boundary within reasonable distance
    const candidateBoundaries = boundaries.filter(
      (b) => b > start && b <= Math.min(targetEnd * 1.2, content.length)
    );

    if (candidateBoundaries.length === 0) {
      return Math.min(targetEnd, content.length);
    }

    // Prefer boundaries closer to target
    return candidateBoundaries.reduce((best, current) =>
      Math.abs(current - targetEnd) < Math.abs(best - targetEnd)
        ? current
        : best
    );
  }

  /**
   * Create contextual metadata for late chunking
   */
  private createContextualMetadata(
    fullDocument: string,
    chunkStart: number,
    chunkEnd: number,
    url: string
  ) {
    const beforeContext = fullDocument
      .slice(Math.max(0, chunkStart - 200), chunkStart)
      .trim();

    const afterContext = fullDocument
      .slice(chunkEnd, Math.min(fullDocument.length, chunkEnd + 200))
      .trim();

    return {
      before: beforeContext ? `...${beforeContext}` : undefined,
      after: afterContext ? `${afterContext}...` : undefined,
      documentLength: fullDocument.length,
    };
  }
}

/**
 * Get default chunking configuration for different use cases
 */
export function getChunkingConfig(
  useCase: "documentation" | "articles" | "code" | "qa"
): ChunkingConfig {
  const baseConfig = {
    chunkOverlap: 50,
    minChunkSize: 100,
    useContextualEmbeddings: true,
  };

  switch (useCase) {
    case "documentation":
      return {
        ...baseConfig,
        strategy: "late-chunking",
        chunkSize: 512,
        maxChunkSize: 800,
        semanticSeparators: ["\n## ", "\n### ", "\n\n", "\n"],
      };

    case "articles":
      return {
        ...baseConfig,
        strategy: "semantic",
        chunkSize: 256,
        maxChunkSize: 512,
        semanticSeparators: ["\n\n", ". ", "! ", "? "],
      };

    case "code":
      return {
        ...baseConfig,
        strategy: "semantic",
        chunkSize: 1024,
        maxChunkSize: 2048,
        semanticSeparators: ["\n\nclass ", "\n\nfunction ", "\n\ndef ", "\n\n"],
      };

    case "qa":
      return {
        ...baseConfig,
        strategy: "sentence",
        chunkSize: 128,
        maxChunkSize: 256,
        chunkOverlap: 20,
      };

    default:
      return {
        ...baseConfig,
        strategy: "late-chunking",
        chunkSize: 512,
        maxChunkSize: 1000,
      };
  }
}
